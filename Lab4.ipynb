{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# Definicion de ambiente\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10 # Pueden cambiar este número si gustan\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None \n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state, reward\n",
    "# Init el ambiente\n",
    "env = InventoryEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar un episodio\n",
    "def generate_episode(env, policy, max_steps=10):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state, action, reward, next_state.copy()))\n",
    "        state = next_state.copy()\n",
    "    return episode\n",
    "\n",
    "# Política de inventario específica (ejemplo simple que reabastece al azar)\n",
    "def random_policy(state):\n",
    "    return {product: random.randint(0, 2) for product in state.keys()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'product_A': 3, 'product_B': 3}, {'product_A': 2, 'product_B': 2}, -9, {'product_A': 9, 'product_B': 6})\n",
      "({'product_A': 9, 'product_B': 6}, {'product_A': 2, 'product_B': 1}, 3, {'product_A': 8, 'product_B': 7})\n",
      "({'product_A': 8, 'product_B': 7}, {'product_A': 0, 'product_B': 0}, 50, {'product_A': 6, 'product_B': 5})\n",
      "({'product_A': 6, 'product_B': 5}, {'product_A': 2, 'product_B': 2}, 11, {'product_A': 6, 'product_B': 6})\n",
      "({'product_A': 6, 'product_B': 6}, {'product_A': 0, 'product_B': 0}, 25, {'product_A': 5, 'product_B': 5})\n",
      "({'product_A': 5, 'product_B': 5}, {'product_A': 1, 'product_B': 1}, 28, {'product_A': 5, 'product_B': 4})\n",
      "({'product_A': 5, 'product_B': 4}, {'product_A': 0, 'product_B': 2}, 16, {'product_A': 5, 'product_B': 4})\n",
      "({'product_A': 5, 'product_B': 4}, {'product_A': 1, 'product_B': 0}, 20, {'product_A': 5, 'product_B': 3})\n",
      "({'product_A': 5, 'product_B': 3}, {'product_A': 1, 'product_B': 0}, 15, {'product_A': 4, 'product_B': 3})\n",
      "({'product_A': 4, 'product_B': 3}, {'product_A': 1, 'product_B': 0}, 15, {'product_A': 3, 'product_B': 3})\n"
     ]
    }
   ],
   "source": [
    "# Generar episodios y recolectar datos\n",
    "env = InventoryEnvironment()\n",
    "episodes = [generate_episode(env, random_policy) for _ in range(100)]\n",
    "\n",
    "# Mostrar un ejemplo de episodio\n",
    "for step in episodes[0]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploring_starts(env, num_episodes=100, max_steps=10):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = {product: random.randint(0, env.max_stock) for product in env.products}\n",
    "        action = {product: random.randint(0, 2) for product in env.products}\n",
    "        episode = []\n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward = env.step(action)\n",
    "            episode.append((state, action, reward, next_state.copy()))\n",
    "            state = next_state.copy()\n",
    "            action = random_policy(state)\n",
    "        episodes.append(episode)\n",
    "    return episodes\n",
    "\n",
    "# Generar episodios con Exploring Starts\n",
    "exploring_episodes = exploring_starts(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'product_A': 1, 'product_B': 5}, {'product_A': 0, 'product_B': 1}, 18, {'product_A': 2, 'product_B': 6})\n",
      "({'product_A': 2, 'product_B': 6}, {'product_A': 1, 'product_B': 2}, 1, {'product_A': 1, 'product_B': 8})\n",
      "({'product_A': 1, 'product_B': 8}, {'product_A': 0, 'product_B': 1}, 3, {'product_A': 0, 'product_B': 9})\n",
      "({'product_A': 0, 'product_B': 9}, {'product_A': 0, 'product_B': 2}, -14, {'product_A': 0, 'product_B': 10})\n",
      "({'product_A': 0, 'product_B': 10}, {'product_A': 1, 'product_B': 0}, 10, {'product_A': 1, 'product_B': 9})\n",
      "({'product_A': 1, 'product_B': 9}, {'product_A': 2, 'product_B': 1}, -2, {'product_A': 3, 'product_B': 9})\n",
      "({'product_A': 3, 'product_B': 9}, {'product_A': 2, 'product_B': 0}, 0, {'product_A': 4, 'product_B': 9})\n",
      "({'product_A': 4, 'product_B': 9}, {'product_A': 0, 'product_B': 2}, 1, {'product_A': 4, 'product_B': 9})\n",
      "({'product_A': 4, 'product_B': 9}, {'product_A': 1, 'product_B': 2}, 1, {'product_A': 3, 'product_B': 10})\n",
      "({'product_A': 3, 'product_B': 10}, {'product_A': 2, 'product_B': 1}, 33, {'product_A': 3, 'product_B': 8})\n"
     ]
    }
   ],
   "source": [
    "for i in exploring_episodes[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return {product: 1 for product in state.keys()}\n",
    "\n",
    "# Generar episodios usando epsilon-greedy\n",
    "def generate_episode_with_epsilon(env, policy, epsilon=0.1, max_steps=10):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(state, epsilon)\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state, action, reward, next_state.copy()))\n",
    "        state = next_state.copy()\n",
    "    return episode\n",
    "\n",
    "epsilon_episodes = [generate_episode_with_epsilon(env, epsilon_greedy_policy) for _ in range(100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'product_A': 3, 'product_B': 1}, {'product_A': 1, 'product_B': 1}, 38, {'product_A': 3, 'product_B': 2})\n",
      "({'product_A': 3, 'product_B': 2}, {'product_A': 1, 'product_B': 1}, 23, {'product_A': 2, 'product_B': 2})\n",
      "({'product_A': 2, 'product_B': 2}, {'product_A': 1, 'product_B': 1}, 8, {'product_A': 1, 'product_B': 3})\n",
      "({'product_A': 1, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 38, {'product_A': 0, 'product_B': 2})\n",
      "({'product_A': 0, 'product_B': 2}, {'product_A': 1, 'product_B': 1}, 13, {'product_A': 0, 'product_B': 2})\n",
      "({'product_A': 0, 'product_B': 2}, {'product_A': 1, 'product_B': 1}, -2, {'product_A': 0, 'product_B': 3})\n",
      "({'product_A': 0, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 13, {'product_A': 0, 'product_B': 3})\n",
      "({'product_A': 0, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 3, {'product_A': 1, 'product_B': 3})\n",
      "({'product_A': 1, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 18, {'product_A': 2, 'product_B': 2})\n",
      "({'product_A': 2, 'product_B': 2}, {'product_A': 1, 'product_B': 1}, 18, {'product_A': 3, 'product_B': 1})\n"
     ]
    }
   ],
   "source": [
    "for i in epsilon_episodes[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Política objetivo (siempre reponer a nivel 1 por ejemplo)\n",
    "def target_policy(state):\n",
    "    return {product: 1 for product in state.keys()}\n",
    "\n",
    "# Generar episodios usando aprendizaje off-policy\n",
    "def generate_off_policy_episode(env, behavior_policy, target_policy, epsilon=0.1, max_steps=10):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    for _ in range(max_steps):\n",
    "        action = behavior_policy(state, epsilon)\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state, action, reward, next_state.copy()))\n",
    "        state = next_state.copy()\n",
    "    return episode\n",
    "\n",
    "off_policy_episodes = [generate_off_policy_episode(env, epsilon_greedy_policy, target_policy) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'product_A': 7, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 18, {'product_A': 8, 'product_B': 0})\n",
      "({'product_A': 8, 'product_B': 0}, {'product_A': 1, 'product_B': 1}, -2, {'product_A': 8, 'product_B': 1})\n",
      "({'product_A': 8, 'product_B': 1}, {'product_A': 1, 'product_B': 1}, 18, {'product_A': 9, 'product_B': 0})\n",
      "({'product_A': 9, 'product_B': 0}, {'product_A': 1, 'product_B': 1}, -2, {'product_A': 9, 'product_B': 1})\n",
      "({'product_A': 9, 'product_B': 1}, {'product_A': 1, 'product_B': 2}, 1, {'product_A': 8, 'product_B': 3})\n",
      "({'product_A': 8, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 23, {'product_A': 7, 'product_B': 3})\n",
      "({'product_A': 7, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 3, {'product_A': 8, 'product_B': 3})\n",
      "({'product_A': 8, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 13, {'product_A': 8, 'product_B': 3})\n",
      "({'product_A': 8, 'product_B': 3}, {'product_A': 1, 'product_B': 1}, 8, {'product_A': 7, 'product_B': 4})\n",
      "({'product_A': 7, 'product_B': 4}, {'product_A': 1, 'product_B': 1}, 28, {'product_A': 7, 'product_B': 3})\n"
     ]
    }
   ],
   "source": [
    "for i in off_policy_episodes[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preguntas a responder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "\n",
    "El valor estimado de mantener diferentes niveles de existencias para cada producto se puede saber a traves de la funcion valor accion \"Q\". Esta determina el valor de un estado y un accion, esto en el problema determininaría el valor para mantener diferentes niveles de existencias para cada prodcuto.\n",
    "\n",
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?\n",
    "\n",
    "El valor de epsilon en una politica blanda determina que tanto se explorara o bien que tanto se explotara. Para un valor epsilon alto, por ejemplo de 0.9, determina que el 90% del tiempo se tomará una acción aleatoria antes de la optima, dando como resultado un estado al que no se ha llegado antes y de esta manera cambiando la politica. Es importante tomar esto en cuenta debido a que puede determinar que tan bien funcionará la politica para lo que se este intentando de abordar y si se podrá llegar a algun optimo debido a esto.  \n",
    "\n",
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro de la política?\n",
    "\n",
    "Mientras que al utilizar un aprendizaje dentro de la politica ofrece una opción suboptima de la acción a tomar, sin embargo puede darse el caso que su exploración puede ser limitada por lo que no llegue a aprender algo optimo. Por otro lado la fuera de politica aprende a seguir la politica que esta estudiando mientras que esta tomando acciones utilizando una distinta politica. Al lograr esto, se puede observar que tan diferentes son las 2 politicas y bien que tan buena es la politica que se esta estudiando para un estado determinado. Un ejemplo parecido a ello podrian ser los amiibos de smash bros, los cuales cuando se ponen en modo de imitación imitan la forma de jugar de la persona que esta jugando lo cual se parece a un aprendizaje fuera de politica. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
